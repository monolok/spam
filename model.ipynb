{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_md -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import en_core_web_md\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import re\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/antoinebertin/Documents/jedha/full_stack/projects_full_stack/spam/spam.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Fit label encoder and return encoded labels\n",
    "encoded_labels = le.fit_transform(df['v1'])\n",
    "df['encoding'] = encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * using the command `str.isalnum` remove all characters from your strings that are not alphanumeric except for whitespaces, and apostrophies.\n",
    " * using `str.replace`, `str.lower` and `str.strip` replace double whitespaces with single whitespaces, convert all characters to lowercase and trim starting and finishing whitespaces.\n",
    " * using spacy, replace all tokens in your texts with `lemma_` and remove all the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove specified characters and symbols\n",
    "    text = re.sub(r\"[.,?!;:*&\\-'/()]\", \"\", text)  # Remove listed characters\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)  # Remove single letters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Lemmatization and removing stopwords\n",
    "    text = \" \".join([token.lemma_ for token in nlp(text) if (token.lemma_ not in STOP_WORDS) and (token.text not in STOP_WORDS)])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"v2_clean\"] = df[\"v2\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are all the v2_clean the same type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v2_clean\n",
       "True    5572\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df.v2_clean.apply(lambda x: type(x)==str)\n",
    "mask.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check most_common words for more preprocessing and prep for tokenizer steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('free', 278), ('ok', 277), ('good', 273), ('send', 270), ('like', 252), ('want', 243), ('ill', 237), ('time', 235), ('day', 234), ('love', 224), ('text', 214), ('tell', 208), ('think', 199), ('need', 185), ('txt', 164), ('home', 162), ('lor', 160), ('reply', 159), ('sorry', 156), ('stop', 155)]\n"
     ]
    }
   ],
   "source": [
    "words = df['v2_clean'].str.lower().str.split(expand=True).stack()\n",
    "\n",
    "# Count words\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Display the most n common words\n",
    "print(word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_words = {'ur', 'come', 'know', 'm', 's', '2', '4', 'lt#gt', '\"'}\n",
    "\n",
    "# Function to remove specific words from a single text\n",
    "def remove_specific_words(text):\n",
    "    return \" \".join(word for word in text.split() if word not in specific_words)\n",
    "\n",
    "# Apply this function to each text in the 'v2_clean' column\n",
    "df['v2_clean'] = df['v2_clean'].apply(remove_specific_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize v2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all words into a single list and then create a set to find unique words\n",
    "unique_words = set(word for message in df.v2_clean for word in message.split())\n",
    "# Calculate the total number of unique words\n",
    "total_unique_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41064, 8364)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = df.v2_clean.apply(lambda x: len(x.split()))\n",
    "total_words.sum(), total_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.text.Tokenizer(num_words=1000, oov_token=\"out_of_vocab\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
