{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_md -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 17:06:17.964404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 17:06:18.990124: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import en_core_web_md\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspaces/spam/spam.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Fit label encoder and return encoded labels\n",
    "encoded_labels = le.fit_transform(df['v1'])\n",
    "df['encoding'] = encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * using the command `str.isalnum` remove all characters from your strings that are not alphanumeric except for whitespaces, and apostrophies.\n",
    " * using `str.replace`, `str.lower` and `str.strip` replace double whitespaces with single whitespaces, convert all characters to lowercase and trim starting and finishing whitespaces.\n",
    " * using spacy, replace all tokens in your texts with `lemma_` and remove all the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove specified characters and symbols\n",
    "    text = re.sub(r\"[.,?!;:*&\\-'/()]\", \"\", text)  # Remove listed characters\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)  # Remove single letters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Lemmatization and removing stopwords\n",
    "    text = \" \".join([token.lemma_ for token in nlp(text) if (token.lemma_ not in STOP_WORDS) and (token.text not in STOP_WORDS)])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"v2_clean\"] = df[\"v2\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are all the v2_clean the same type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v2_clean\n",
       "True    5572\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df.v2_clean.apply(lambda x: type(x)==str)\n",
    "mask.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check most_common words for more preprocessing and prep for tokenizer steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2', 480), ('m', 463), ('ur', 380), ('come', 322), ('4', 287), ('know', 283), ('s', 283), ('free', 278), ('ok', 277), ('lt#gt', 276), ('good', 273), ('send', 270), ('like', 252), ('want', 243), ('ill', 237), ('time', 235), ('day', 234), ('\"', 232), ('love', 224), ('text', 214)]\n"
     ]
    }
   ],
   "source": [
    "words = df['v2_clean'].str.lower().str.split(expand=True).stack()\n",
    "\n",
    "# Count words\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Display the most n common words\n",
    "print(word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_words = {'ur', 'come', 'know', 'm', 's', '2', '4', 'lt#gt', '\"'}\n",
    "\n",
    "# Function to remove specific words from a single text\n",
    "def remove_specific_words(text):\n",
    "    return \" \".join(word for word in text.split() if word not in specific_words)\n",
    "\n",
    "# Apply this function to each text in the 'v2_clean' column\n",
    "df['v2_clean'] = df['v2_clean'].apply(remove_specific_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize v2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all words into a single list and then create a set to find unique words\n",
    "unique_words = set(word for message in df.v2_clean for word in message.split())\n",
    "# Calculate the total number of unique words\n",
    "total_unique_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41064, 8364)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = df.v2_clean.apply(lambda x: len(x.split()))\n",
    "total_words.sum(), total_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000, oov_token=\"out_of_vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the index for each words\n",
    "tokenizer.fit_on_texts(df['v2_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to find the best num_words because if set too low some rare words will not be learned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique out_of_vocab words: 249\n",
      "Unique out_of_vocab words: {'\\\\not', '\\\\for', '\\\\nonenowhere', 'joy\\\\', 'doesdiscountshitinnit\\\\', '\\\\hello@drivby0quit', '\\\\jeevithathile', '\\\\how', '\\\\\"she', 'life\\\\', 'late\\\\', '\\\\wen', '\\\\urgent', '\\\\gran', '\\\\smokes', '\\\\are', 'xxx\\\\', '\\\\gette', 'forever\\\\', '\\\\goodmorne', '\\\\shit', 'more\\\\', 'drive\\\\', 'wrk\\\\', 'credit\\\\', '@', 'adress\\\\', 'later\\\\', '\\\\nver', 'frndship\\\\', '\\\\sweet\\\\', '_', 'couch\\\\', 'xxxx\\\\', '\\\\hi', 'it+both', 'don\\\\', '\\\\\"welcomes\\\\', 'monthlysubscription@50pmsg', '\\\\valentines', '@kiosk', 'shit\\\\', '\\\\the', '\\\\get', '\\\\kudi\\\\\"yarasu', 'everything\\\\', '<', '\\\\pete', '\\\\im', '\\\\hurt', '\\\\oh', 'it\\\\', '\\\\ur', 'alone\\\\', '[', '=', '0a$network', '\\\\power', '\\\\checkmate\\\\', '\\\\hello\\\\', '\\\\\"value', 'energy\\\\', 'crore\\\\', 'terry\\\\', 'msg@å£150rcvd', '\\\\speak', '\\\\it', 'ukp>2000', 'locaxx\\\\', 'l8r\\\\', '\\\\path', '@shesil', '\\\\what', 'mrng\\\\', '\\\\yeh', '\\\\crazy\\\\', 'love|', '+123', 'standing|', 'ì_ll', 'iwas+marinethatåõs', '\\\\dont', 'red]text[colour]txtstar', '@warner', '\\\\ah', '\\\\wow', 'idiot\\\\', 'httpalto18coukwavewaveaspo=44345', 'videosounds+2', '2=', 'honi\\\\', '#', 'got\\\\', ']', 'olowoyey@', 'call2optoutf4q=', '\\\\go', 'friendship\\\\', '\\\\\"life', '\\\\aww', 'katexxx\\\\', '\\\\alright', '\\\\gimme', '>', 'doesn\\\\', 'wmlid=1b6a5ecef91ff937819first', 'up+not', '\\\\thinke', '\\x89û_thanks', 'dead\\\\', 'xclusive@clubsaisai', '18+only', '\\\\\\\\', 'initiate\\\\', '\\\\\"b', '\\\\sleep', 'update_now', '3=', '\\\\cheer', 'recd@thirtyeight', '\\\\\"margaret', '\\\\\"how', '\\\\keep', 'barbie\\\\', '\\\\julianaland\\\\', 'info@vipclub4u', '\\\\woah\\\\', '\\\\\"gud', 'login=', '\\\\usf', 'wwwshortbreaksorguk\\\\', 'nytec2a3lpmsg@150p', 'wmlid=820554ad0a1705572711first', '\\\\stop', '11mths+', 'customersqueries@netvisionukcom', 'info@ringtonekingcouk', '\\\\hello', '$', '\\\\walk', 'isaiah=', '^', '\\\\\"you\\\\', 'me\\\\', '|', '16+only', '50\\\\', '\\\\cha', '\\\\our', '\\\\\"with', 'answer\\\\', '\\\\life', '\\\\\"1u', '\\\\ey', 'hello\\\\', '\\\\polys\\\\', '\\\\divorce', 'seperated\\x8eö´\\x89ó_\\x8bû¬ud', '\\\\boo', '\\\\\"shah', 'ppt150x3+normal', 'mix\\\\', '\\\\find', 'xxxxxxx\\\\', '\\\\3000', 'msgs@150p', 'forgiveness\\\\', '\\\\adp\\\\', '\\\\drink\\\\', '\\\\\"the', '\\\\me', '\\\\alrite', '\\\\\"drive', 'few\\\\', '\\\\petey', '31pmsg@150p', '\\\\happy', '\\\\\"this', 'tear\\\\', '~', 'nic\\\\', 'meet+greet', 'moral\\\\dont', '\\\\sometime', 'persons\\\\', 'xxxxx\\\\', '\\\\song', 'gm+gn+ge+gn', 'charged@150pmsg2', '+', '\\\\hey', 'pple$700', 'logos+musicnew', 'sneham\\\\', 'week|', '\\\\\"morning\\\\', '\\\\response\\\\', 'msg+ticket@kioskvalid', 'need|', 'here\\\\', 'big|', '\\\\wylie', 'maat\\\\', 'dorothy@kiefercom', 'tddnewsletter@emc1couk', '\\\\boost', '||', '\\\\suppliers\\\\', 'cave\\\\', 'week+', 'today\\\\', '\\\\\"enjoy\\\\', '+447797706009', 'said\\\\if', '\\\\miss', '\\\\\"oh\\\\', 'pdate_now', 'bedrm$900', '\\\\symptoms\\\\', 'silence\\\\', '\\\\er', 'chat\\\\', 'stops\\\\', 'info@txt82228couk', 'day\\\\', '\\\\be', 'house\\\\', '\\\\sicomo', 'fine\\\\', 'yijue@hotmailcom', '+449071512431', 'much\\\\', '\\\\mix\\\\', 'padhegm\\\\', 'nite+2', 'waiting\\\\', '\\\\because', 'frnds\\\\', 'stop\\\\', '\\\\can', 'people\\\\', '4=', '\\\\\"our', '88039skilgmetscs087147403231winawkage16+å£150perwksub', 'chrgd@50p', '\\\\', '%', 'hello\\\\you', '5='}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer's learned vocabulary\n",
    "learned_words = set(tokenizer.word_index.keys())\n",
    "\n",
    "# Dataset's vocabulary\n",
    "dataset_words = set(word for text in df['v2_clean'] for word in text.lower().split())\n",
    "\n",
    "# Find the unique OOV words by comparing the two sets\n",
    "unique_oov_words = dataset_words - learned_words\n",
    "\n",
    "print(f\"Number of unique out_of_vocab words: {len(unique_oov_words)}\")\n",
    "print(f\"Unique out_of_vocab words: {unique_oov_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"v2_encoded\"] = tokenizer.texts_to_sequences(df['v2_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>encoding</th>\n",
       "      <th>v2_clean</th>\n",
       "      <th>v2_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>jurong point crazy available bugis great world...</td>\n",
       "      <td>[3606, 213, 424, 442, 921, 40, 187, 922, 2352,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joke wif oni</td>\n",
       "      <td>[3, 177, 443, 270, 1437]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts 21s...</td>\n",
       "      <td>[2, 283, 510, 642, 22, 1438, 822, 402, 1439, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>dun early hor</td>\n",
       "      <td>[109, 132, 2356]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>nah think usf live</td>\n",
       "      <td>[683, 14, 643, 114]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2  encoding  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...         0   \n",
       "1   ham                      Ok lar... Joking wif u oni...         0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...         1   \n",
       "3   ham  U dun say so early hor... U c already then say...         0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...         0   \n",
       "\n",
       "                                            v2_clean  \\\n",
       "0  jurong point crazy available bugis great world...   \n",
       "1                                ok lar joke wif oni   \n",
       "2  free entry wkly comp win fa cup final tkts 21s...   \n",
       "3                                      dun early hor   \n",
       "4                                 nah think usf live   \n",
       "\n",
       "                                          v2_encoded  \n",
       "0  [3606, 213, 424, 442, 921, 40, 187, 922, 2352,...  \n",
       "1                           [3, 177, 443, 270, 1437]  \n",
       "2  [2, 283, 510, 642, 22, 1438, 822, 402, 1439, 1...  \n",
       "3                                   [109, 132, 2356]  \n",
       "4                                [683, 14, 643, 114]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets make sure all the v2_encoded have the same lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.preprocessing.sequence.pad_sequences(df['v2_encoded'], padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['encoding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have imbalanced -> stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this will help use built in tensorflow method to shuffle and batch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val = tf.data.Dataset.from_tensor_slices((x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = train.shuffle(len(train)).batch(64)\n",
    "val_batch = val.shuffle(len(val)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 190   75 1409 ...    0    0    0]\n",
      " [ 896  111    0 ...    0    0    0]\n",
      " [  21    9   28 ...    0    0    0]\n",
      " ...\n",
      " [1060  828  548 ...    0    0    0]\n",
      " [1133  893    0 ...    0    0    0]\n",
      " [   2   60 6839 ...    0    0    0]], shape=(64, 59), dtype=int32) tf.Tensor(\n",
      "[1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1], shape=(64,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 17:08:31.320459: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    " # Regardons un batch \n",
    "for X, y in train_batch.take(1):\n",
    "  print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
